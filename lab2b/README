NAME: Robert Geil
EMAIL: rgeil@ucla.edu
ID: 104916969


Questions:
2.3.1 - CPU time in the basic list implementation:
    Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests?
        I think that in the one and two thread lists, the majority of the CPU execution
        time is spent on list traversal, rather than on the insertion and deletion,
        especially as the list grows.
    Why do you believe these to be the most expensive parts of the code?
        I believe that list traversal is more expensive that insertion and deletion, because
        to insert and/or delete, once the node has been located, it is only a few machine
        instructions to switch around the pointers. However, in order to get to those locations,
        up to N instructions may be needed, where N is the length of the list. In addition, most
        time isn't going to synchronization of the threads because with only one or two threads
        active at a given moment, there isn't much contention for locks, so most threads are doing
        meaningful work.
    Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
        In the high-thread spin-lock test case, I think that most of the CPU time is going to 
        "spinning" while waiting for the lock to be acquired. Since the entire list is locked when
        any given thread is accessing, inserting or deleting, all other threads, when set by the
        scheduler, simply spin, waiting for the list to be unlocked. This contributes to the much
        lower throughput as the number of threads increases, as there are more threads that are simply
        spinning while waiting for their turn.
    Where do you believe most of the CPU time is being spent in the high-thread mutex tests?
        I believe that in high-thead mutex tests, a large amount of CPU time is used for context
        switching between threads. When pthread_mutex_lock() is unable to acquire the lock, the
        thread is blocked, and therefore must wait for another thread to unlock. The thread that
        is attempting to lock will then yield, causing a context switch for another thread to run.
        However, because the scheduler naively chooses another thread to run, it is also likely that
        that thread is locked, meaning that several context switches may be needed before whichever
        thread that currently holds the list is choosen and can continue execution. In addition, 
        because only one thread can hold the lock, as the number of threads increase, an issue of
        scaling follows, lowering throughput as a function of thread count.

2.3.2 - Execution Profiling:
    Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the 
    list exerciser is run with a large number of threads?
        When running my profiler with the --text option (and --threads=12, --sync=s), it shows that
        the add_to_list function takes the most CPU time. Breaking down that function with 
        --list=add_to_list, we see that more than three quarters of the time (in add_to_list) is spent 
        on line 374, or the "spinning" line of the spin lock, where the threads attempt to acquire 
        the lock. 
    Why does this operation become so expensive with large numbers of threads?
        As the number of threads increases, there is more lock-contention. Since only one thread can
        hold the lock at a time, as the number of threads increase, each thread spends more time
        hammering on the CPU attempting to reacquire the lock, and the amount of time spent on
        line 374 (and the other spots in the program where locks are used) increases.
2.3.3 - Mutex Wait Time:
    Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
    Why does the average lock-wait time rise so dramatically with the number of contending threads?
        As the number of threads increases, the wait time increases more sharply than linear growth. This is
        due to the fact that as thread count increases, there is a two-fold slowdown on the computations.
        One part is that more threads means that the chance of having to wait increases, since there is still only
        one thread executing at a time. In addition, more threads means there is a larger queue of threads,
        so once a thread loses the lock, there is a longer line to wait in to reacquire it.
    Why does the completion time per operation rise (less dramatically) with the number of contending threads?
        Since completion time is based on number of operations completed, increasing threads doesn't have as
        dramatic an impact on the results. For example, while an individual thread may have to wait longer to
        do work, there is still roughly one thread running at a time, meaning that the only slowdown for 
        completion time is context switching.
    How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
        Even with multiple threads, there is still generally one thread doing meaningful work. Since mutex locks
        cause threads to yield as soon as they cannot acquire a lock, context switches increase as thread increase,
        and wait time goes up dramatically. However, the completion time is lower because meaningful work is still
        being done, albeit slowed by context switches.
    