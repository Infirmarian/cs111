NAME: Robert Geil
EMAIL: rgeil@ucla.edu
ID: 104916969

Lab 2A - Races and Synchronization

Included files:
    - lab2_list.c
        Source file for a multithreaded linked list update program
    - SortedList.c
        Source file for an implementation of a sorted linked list
    - SortedList.h
        Header file for SortedList.c
    - utils.c
        Source file for utilities that were seperated out from lab2_list.c
    - utils.h
        Header file for utils.c
    - Makefile
        Makefile with options to make distribution (make dist), run checks (make tests), generate graphs (make graphs)
        and clean up (make clean)
    - README
        A text file containing a description of program functionality and answers to questions
    - gen_csv.sh
        A shell script to run a series of tests on lab2_list
    - lab2b_list.gp
        A gnuplot program to generate a series of plots based on data from lab2_list
    - lab2b_list.csv
        A CSV file containing the results of the tests from lab2_list
    - lab2b_[1-5].png
        A 5 PNG images containing the plotted results of lab2b_list.csv
    - profile.out
        A result of gperftools analysis on the lab2_list

Program Description:
    lab2_list:
        This program performs insertions, lookup, deletions and length calculations on a shared linked list, with or without locks
        and with the option of a series of sublists
        Options:
        --iterations=N
            Insert and delete N elements into the list
        --threads=N
            Create N threads to perform insertions and deletions
        --yield=[idl]
            Yield the CPU between each i=insertions, d=deletion, l=lookup. These can be specified in any combination
        --sync=[ms]
            Synchronization each thread with either a m=Mutex or s=Spin lock to prevent race conditions
        --lists=N
            Break up the list into N sublists, improving multithread performance
        
        Output:
        This program either generates correct output, as shown below, or in the case of a corrupted list (due to a race condition)
        prints out an error message and immediately exits with code 2. Normal output is in the form
        name of test,#threads,#iterations,#total operations,total runtime,runtime per operation,wait time per operation

    Return codes for both functions are as follows:
        0: Success
        1: Failed system call or invalid command line argument
        2: Other error (especially corrupted list due to race conditions)

Limitations and Features:
    One feature of this program is good memory management. Using valgrind, this program was checked for leaks under most normal conditions,
    meaning that all allocated memory is freed just before exit of the program. Some limitations are the lack of an independent testing
    script, meaning that the provided sanity check has to suffice for all testing. In addition, the structure of the Makefile could be
    improved for clarity.

Documentation from https://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Atomic-Builtins.html was used in order to run synchronization operations,
and extensive man page documentation was used, especially for pthreads and malloc. The hashing algorithm for breaking up
lists into sublists was the djb2 algorithm, described at http://www.cse.yorku.ca/~oz/hash.html.



Questions:
2.3.1 - CPU time in the basic list implementation:
    Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests?
        I think that in the one and two thread lists, the majority of the CPU execution
        time is spent on list traversal, rather than on the insertion and deletion,
        especially as the list grows.
    Why do you believe these to be the most expensive parts of the code?
        I believe that list traversal is more expensive that insertion and deletion, because
        to insert and/or delete, once the node has been located, it is only a few machine
        instructions to switch around the pointers. However, in order to get to those locations,
        up to N instructions may be needed, where N is the length of the list. In addition, most
        time isn't going to synchronization of the threads because with only one or two threads
        active at a given moment, there isn't much contention for locks, so most threads are doing
        meaningful work.
    Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
        In the high-thread spin-lock test case, I think that most of the CPU time is going to 
        "spinning" while waiting for the lock to be acquired. Since the entire list is locked when
        any given thread is accessing, inserting or deleting, all other threads, when set by the
        scheduler, simply spin, waiting for the list to be unlocked. This contributes to the much
        lower throughput as the number of threads increases, as there are more threads that are simply
        spinning while waiting for their turn.
    Where do you believe most of the CPU time is being spent in the high-thread mutex tests?
        I believe that in high-thead mutex tests, a large amount of CPU time is used for context
        switching between threads. When pthread_mutex_lock() is unable to acquire the lock, the
        thread is blocked, and therefore must wait for another thread to unlock. The thread that
        is attempting to lock will then yield, causing a context switch for another thread to run.
        However, because the scheduler naively chooses another thread to run, it is also likely that
        that thread is locked, meaning that several context switches may be needed before whichever
        thread that currently holds the list is choosen and can continue execution. In addition, 
        because only one thread can hold the lock, as the number of threads increase, an issue of
        scaling follows, lowering throughput as a function of thread count.

2.3.2 - Execution Profiling:
    Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the 
    list exerciser is run with a large number of threads?
        When running my profiler with the --text option (and --threads=12, --sync=s), it shows that
        the add_to_list function takes the most CPU time. Breaking down that function with 
        --list=add_to_list, we see that more than three quarters of the time (in add_to_list) is spent 
        on line 374, or the "spinning" line of the spin lock, where the threads attempt to acquire 
        the lock. 
    Why does this operation become so expensive with large numbers of threads?
        As the number of threads increases, there is more lock-contention. Since only one thread can
        hold the lock at a time, as the number of threads increase, each thread spends more time
        hammering on the CPU attempting to reacquire the lock, and the amount of time spent on
        line 374 (and the other spots in the program where locks are used) increases.
2.3.3 - Mutex Wait Time:
    Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
    Why does the average lock-wait time rise so dramatically with the number of contending threads?
        As the number of threads increases, the wait time increases more sharply than linear growth. This is
        due to the fact that as thread count increases, there is a two-fold slowdown on the computations.
        One part is that more threads means that the chance of having to wait increases, since there is still only
        one thread executing at a time. In addition, more threads means there is a larger queue of threads,
        so once a thread loses the lock, there is a longer line to wait in to reacquire it.
    Why does the completion time per operation rise (less dramatically) with the number of contending threads?
        Since completion time is based on number of operations completed, increasing threads doesn't have as
        dramatic an impact on the results. For example, while an individual thread may have to wait longer to
        do work, there is still roughly one thread running at a time, meaning that the only slowdown for 
        completion time is context switching.
    How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
        Even with multiple threads, there is still generally one thread doing meaningful work. Since mutex locks
        cause threads to yield as soon as they cannot acquire a lock, context switches increase as thread increase,
        and wait time goes up dramatically. However, the completion time is lower because meaningful work is still
        being done, albeit slowed by context switches.
2.3.4 - Performance of Partitioned Lists
    Explain the change in performance of the synchronized methods as a function of the number of lists.
        As the list is broken into more pieces, the bottlenecking of a lock is distributed across the lists.
        With multiple lists, multiple threads can do useful work at the same time, increasing throughput,
        since the whole list doesn't need to be controlled by a single thread.
    Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
        Yes, there is a twofold reason for the increase in throughput as lists increase. Firstly, more lists
        means that there is a lower probability of lock contention, where two threads attempt to acquire the same
        lock (since the keys inserted are randomly generated, and the hashing function is good for distribution).
        In addition, breaking up the list decreases traversal time to insert and lookup, as the complexity is
        N/L, where N is the number of elements, and L the number of sublists, as compared to a complexity on the
        order of N for just a single list.
    It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to 
    the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? 
    If not, explain why not.
        No, this isn't quite the case. Because with multiple lists there is still a chance for contention for
        locks, there are still collisions that occur when inserting, meaning that the performance is still lower
        than 1/N threads with a single list.
    