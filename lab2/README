NAME: Robert Geil
EMAIL: rgeil@ucla.edu
ID: 104916969

Questions:
2.1.1 - causing conflicts:
    Why does it take many iterations before errors are seen?
        With a small number of iterations (100), the tests provided a correct value of the counter,
        when the number of threads was less than 10. Beyond that, for 1000+ iterations, all multi-threaded
        executions generated an incorrect value, at least in these tests. It takes a large number of
        iterations to cause errors because the chance of an individual error occurring is relatively low.
        The point where the error occurs is between the load-add-store operations on the pointer value.
        That means there are only 2 machine instructions where the value can be corrupted. This means that for
        small values of iterations, it is very possible that no context switch occurs between the load-add-store,
        meaning the program behaves "correctly". Adding more iterations makes it more likely that a race occurs.
    Why does a significantly smaller number of iterations so seldom fail?
        When there are only a few iterations, there are very few chances for a context switch to interrupt the
        thread execution. For example, with 8 threads and 100 iterations, there are only ~3200 gaps in machine
        instructions where an interrupt can occur to mess with race. However, increasing to 1000 iterations gives
        32,000 opportunities for a context switch, which increases the likelihood of an error.
2.1.2 - cost of yielding:
    Why are the --yield runs so much slower?
        Yield runs much slower because, in addition to the main program work of incrementing and decrementing a variable,
        an additional system call to yield needs to be made for every single instruction, greatly increasing the overhead of the program.
        Yield makes more sense when running some external task, such as waiting for IO to complete, rather than 
        adding to a variable on the heap. In this sense, yield is only increasing the chances of a race between the threads.
    Where is the additional time going?
        The additional time is going to context switching and processes run by the scheduler. By yielding more frequently than
        interrupts would normally occur, more time is spent switching contexts and within the system call of yield, rather
        than completing useful work by each of the threads.
    Is it possible to get valid per-operation timings if we are using the --yield option? If so, explain how. If not, explain why not.
        Yes, it is possible, but not practical. In order to get accurate per-operation timings, the amount of time that the yield takes
        would have to subtracted from the total time. This could be accomplished by recording the time before and after the call to 
        yield, but this additional overhead may render the timing sequence less meaningful than before, as the call to clock_gettime
        is quite slow.
2.1.3 - measurement errors:
    Why does the average cost per operation drop with increasing iterations?
        Average cost per operation drops with increasing iterations because the overhead of spawning and joining threads is
        amortized over the larger number of operations. For example, say that spawning and joining threads takes a fixed time of
        1000 units, which each operation takes 1 unit of time. By doing 100 operations on 2 threads, there would be a total number
        of operations equal to 1*100*2*2 = 400, plus the overhead, giving a total time of 1400 units. Dividing by the 400 operations gives
        an average time per op of ~4, which is 4 times the actual time. However, by increasing the number of operations to 10000, the total
        time would be 1*10000*2*2 + 1000 = 41,000, which divided by 40,000 operations gives an average time per operation of 1.025, or 
        much closer to the actual time value, because the cost of the overhead has been amortized.
    If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the "correct" cost is)?
        Because the cost of the overhead is amortized over more and more runs, the more iterations, the closer to the actual value
        we will get. Theoretically, completing an infinite number of iterations would get exactly the time per operation. Since
        this isn't feasible on real machines, a "large" value, several orders of magnitude greater than the time of initializing the threads
        would suffice. It also should be sufficiently far from long long's max value to prevent overflow, as overflowing through race conditions
        will not provide as much meaningful information.